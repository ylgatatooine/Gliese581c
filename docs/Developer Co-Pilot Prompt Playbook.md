# ğŸ§‘â€ğŸ’» Developer Co-Pilot Prompt Playbook

> **Purpose**: Use LLMs to *enhance* your workflow, not replace it. Pair your engineering skills with generative AI to build, test, debug, and ship better software.

## ğŸ“„ Table of Contents

1. [ğŸš¦ Test-Driven Development (TDD)](#-1-test-driven-development-tdd)
2. [ğŸªµ Observability-Driven Debugging](#-2-observability-driven-debugging)
3. [ğŸ§© Chain-of-Thought Prompting](#-3-chain-of-thought-cot)
4. [ğŸ—ƒï¸ Git & GitHub + MCP](#-4-git--github--mcp)
5. [ğŸ§  Meta Prompt Frameworks](#-5-meta-prompt-frameworks)
6. [ğŸ¯ Few-Shot Prompting](#-6-few-shot-prompting)
7. [âš–ï¸ LLM as a Judge](#-7-llm-as-a-judge)
8. [ğŸ› ï¸ Putting It All Together](#ï¸-putting-it-all-together)

---

## ğŸš¦ 1. Test-Driven Development (TDD)

**Why?**  
- Forces clarity on expected behavior  
- Prevents over-engineering  
- Helps you validate edge cases up front  

**Prompt Templates**  
- â€œWrite unit tests for a function that [describe function behavior].â€  
- â€œSuggest edge cases and boundary tests for this [describe feature].â€  
- â€œGiven these tests, what code would satisfy them?â€  
- â€œLetâ€™s do test-driven. Iâ€™ll give requirements, you write the tests first, then weâ€™ll iterate.â€  

**Example**  

```python
# Prompt to LLM:
"Write tests for a function that validates credit card numbers."

# LLM output:
def test_valid_visa():
    assert validate_card("4111111111111111") == True

def test_invalid_length():
    assert validate_card("1234") == False

def test_non_numeric():
    assert validate_card("abcdabcdabcdabcd") == False
````

---

## ğŸªµ 2. Observability-Driven Debugging

**Why?**

* Many bugs are runtime-only
* Logging helps track dynamic state
* Faster root-cause analysis

**Prompt Templates**

* â€œAdd logs to show inputs and outputs for this method.â€
* â€œPlease suggest logging to help isolate a bug in this function.â€
* â€œHereâ€™s the log output. What is your hypothesis now?â€

**Example**

```python
logger.info(f"Fetching data from URL: {url}")
logger.debug(f"Request params: {params}")
logger.error(f"Exception while fetching data: {e}")
```

---

## ğŸ§© 3. Chain-of-Thought (CoT)

**Why?**

* Forces the LLM to â€œthink out loudâ€
* You can catch reasoning errors mid-way
* Improves transparency for tricky bugs

**Prompt Templates**

* â€œThink step by step before suggesting a fix.â€
* â€œExplain each line of this code, then reason about the bug.â€
* â€œWalk through how this algorithm works before rewriting it.â€

**Example**

```plaintext
# Prompt:
"Explain why this binary search fails before fixing it."

# LLM:
1) It initializes left = 0 and right = len(arr)
2) Mid is correct
3) But right = len(arr) causes out-of-bounds
Fix: right = len(arr) - 1
```

---

## ğŸ—ƒï¸ 4. Git & GitHub + MCP

**Why?**

* Standardizes commits
* Makes PR reviews consistent
* Works well with GitHub agents

**Prompt Templates**

* â€œGenerate a commit message summarizing these changes.â€
* â€œWrite a PR description with testing instructions.â€
* â€œWhat tests should reviewers focus on for this PR?â€
* â€œSuggest a semantic version bump for these changes.â€

**Example**

```markdown
# Prompt:
"Write a PR description for this fix."

# LLM:
Title: Fix division-by-zero error in calculate_ratio

Description:
- Adds input validation to avoid division by zero
- Includes test cases for zero denominator

Testing:
- Run `pytest test_math.py`
```

---

## ğŸ§  5. Meta Prompt Frameworks

**Why?**

* Provides consistent conversations
* Easier to scale prompts across a team
* Reduces â€œhallucinationsâ€

**Reusable Template**

```plaintext
[You are a senior software assistant. Please follow this framework:]
1) Identify the userâ€™s task
2) Break it into subproblems
3) Clarify any missing details
4) Solve one subproblem at a time
5) Summarize progress and next steps
```

**Example**

```plaintext
# Prompt:
"Use the meta framework to help design a payment gateway."

# LLM:
1) Task: Build a payment gateway API
2) Subproblems:
   - authentication
   - transaction handling
   - currency support
3) Missing details: Supported currencies? Auth methods?
...
```

---

## ğŸ¯ 6. Few-Shot Prompting

**Why?**

* Shows the LLM your style
* Anchors behavior to consistent examples
* Prevents random answers

**Prompt Templates**

* â€œHere are 3 correct patterns and 1 incorrect. Explain why the incorrect one is wrong.â€
* â€œUse these examples to generate the next function.â€
* â€œRate these implementations from best to worst and explain why.â€

**Example**

```plaintext
# Prompt:
"Given these working patterns, generate a new test."

Examples:
input: [2,4,6] => output: even numbers
input: [1,3,5] => output: empty
input: [0,-2] => output: even numbers

# LLM:
input: [7,8,9] => output: [8]
```

---

## âš–ï¸ 7. LLM as a Judge

**Why?**

* Consistent code reviews
* Surfaces best practices
* Gives objective-style feedback

**Prompt Templates**

* â€œAct as a senior reviewer and rate this code on clarity, performance, and maintainability.â€
* â€œWhich of these two code samples is better, and why?â€
* â€œWhat best practices are violated here?â€

**Example**

```plaintext
# Prompt:
"Review this function for maintainability."

# LLM:
Rating: 6/10

Issues:
- Variable names unclear
- Deeply nested logic
- Missing input validation

Suggestions:
- Rename variables for clarity
- Use guard clauses
```

---

# ğŸ› ï¸ Putting It All Together

âœ… **Write tests first (TDD)**
âœ… **Generate code to pass the tests**
âœ… **Add logs to improve observability**
âœ… **Use chain-of-thought to reason about bugs**
âœ… **Employ GitHub workflows to track changes**
âœ… **Leverage meta prompt frameworks for repeatable consistency**
âœ… **Ask the LLM to judge and review your code**

**Example Combo Prompt**

```plaintext
"Let's write tests for this function, then write code to pass them, then add logs, then review the code with chain-of-thought reasoning. Iâ€™ll feed you logs if it fails, and finally help me write the PR and do a review."
```

---
